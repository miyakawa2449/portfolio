"""
Utility Functions - ä¸€èˆ¬çš„ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
"""
import bleach
from flask import current_app

def sanitize_html(content):
    """HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º"""
    allowed_tags = ['p', 'br', 'strong', 'em', 'u', 'ol', 'ul', 'li', 'a', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']
    allowed_attributes = {'a': ['href', 'title']}
    return bleach.clean(content, tags=allowed_tags, attributes=allowed_attributes, strip=True)

def clear_ogp_cache():
    """OGPã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹é–¢æ•°"""
    from app import ogp_cache
    ogp_cache.clear()
    current_app.logger.info("ğŸ—‘ï¸ OGP cache cleared")

def perform_search(query, search_filter='all'):
    """ã‚µã‚¤ãƒˆå†…æ¤œç´¢ã‚’å®Ÿè¡Œ"""
    from models import Article, Project, db
    from sqlalchemy import select, or_
    
    if not query or len(query.strip()) < 2:
        return {'articles': [], 'projects': [], 'total': 0}
    
    query = query.strip()
    results = {'articles': [], 'projects': [], 'total': 0}
    
    try:
        if search_filter in ['all', 'articles']:
            # è¨˜äº‹æ¤œç´¢ï¼ˆLIKEæ¼”ç®—å­ä½¿ç”¨ï¼‰
            article_query = select(Article).where(
                Article.status == 'published'
            ).where(
                or_(
                    Article.title.like(f'%{query}%'),
                    Article.summary.like(f'%{query}%'),
                    Article.content.like(f'%{query}%')
                )
            ).order_by(Article.published_at.desc())
            
            articles = db.session.execute(article_query).scalars().all()
            results['articles'] = articles
        
        if search_filter in ['all', 'projects']:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¤œç´¢
            project_query = select(Project).where(
                Project.status == 'active'
            ).where(
                or_(
                    Project.title.like(f'%{query}%'),
                    Project.description.like(f'%{query}%'),
                    Project.summary.like(f'%{query}%')
                )
            ).order_by(Project.created_at.desc())
            
            projects = db.session.execute(project_query).scalars().all()
            results['projects'] = projects
        
        results['total'] = len(results['articles']) + len(results['projects'])
        current_app.logger.info(f"ğŸ” Search completed: query='{query}', filter='{search_filter}', total={results['total']}")
        
    except Exception as e:
        current_app.logger.error(f"âŒ Search error: {e}")
        results = {'articles': [], 'projects': [], 'total': 0, 'error': str(e)}
    
    return results

def generate_table_of_contents(markdown_content):
    """Markdownã‹ã‚‰ç›®æ¬¡ã‚’ç”Ÿæˆ"""
    import re
    
    if not markdown_content:
        return None
    
    # è¦‹å‡ºã—ã‚’æŠ½å‡ºã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ# ## ### #### ##### ######ï¼‰
    heading_pattern = r'^(#{1,6})\s+(.+)'
    headings = []
    heading_counter = 0
    
    for line_num, line in enumerate(markdown_content.split('\n'), 1):
        line = line.strip()
        match = re.match(heading_pattern, line)
        if match:
            heading_counter += 1  # è¦‹å‡ºã—ã®é †ç•ªã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            level = len(match.group(1))  # # ã®æ•°
            title = match.group(2).strip()
            
            # ã‚¢ãƒ³ã‚«ãƒ¼ç”¨ã®IDã‚’ç”Ÿæˆï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
            anchor_id = re.sub(r'[^\w\-_\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', '-', title.lower())
            anchor_id = re.sub(r'-+', '-', anchor_id).strip('-')
            anchor_id = f"heading-{heading_counter}-{anchor_id}" if anchor_id else f"heading-{heading_counter}"
            
            headings.append({
                'level': level,
                'title': title,
                'anchor': anchor_id,
                'line': line_num
            })
    
    return headings if headings else None

def add_heading_anchors(html_content):
    """HTMLè¦‹å‡ºã—ã«ã‚¢ãƒ³ã‚«ãƒ¼IDã‚’è¿½åŠ ï¼ˆç›®æ¬¡ã¨ä¸€è‡´ã•ã›ã‚‹ï¼‰"""
    if not html_content:
        return html_content
    
    import re
    from bs4 import BeautifulSoup
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        heading_counter = 0
        
        # h1ã€œh6ã‚¿ã‚°ã‚’é †ç•ªã«æ¤œç´¢ã—ã¦IDã‚’è¿½åŠ 
        for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            heading_counter += 1
            title = heading.get_text().strip()
            
            # ç›®æ¬¡ç”Ÿæˆã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ã§ã‚¢ãƒ³ã‚«ãƒ¼IDã‚’ç”Ÿæˆ
            anchor_id = re.sub(r'[^\w\-_\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', '-', title.lower())
            anchor_id = re.sub(r'-+', '-', anchor_id).strip('-')
            anchor_id = f"heading-{heading_counter}-{anchor_id}" if anchor_id else f"heading-{heading_counter}"
            
            heading['id'] = anchor_id
            current_app.logger.debug(f"Added anchor: {anchor_id} for heading: {title}")
        
        return str(soup)
        
    except Exception as e:
        current_app.logger.error(f"Error adding heading anchors: {e}")
        return html_content

def generate_article_structured_data(article):
    """è¨˜äº‹ç”¨ã®JSON-LDæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    import json
    from datetime import datetime
    
    try:
        # ä½œæˆè€…æƒ…å ±ã‚’å®‰å…¨ã«å–å¾—
        author_name = "Unknown Author"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        if hasattr(article, 'user') and article.user:
            if hasattr(article.user, 'display_name') and article.user.display_name:
                author_name = article.user.display_name
            elif hasattr(article.user, 'email') and article.user.email:
                author_name = article.user.email
        
        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—
        categories = []
        if hasattr(article, 'categories') and article.categories:
            categories = [cat.name for cat in article.categories if hasattr(cat, 'name')]
        
        # è¨˜äº‹ã®æ–‡å­—æ•°ã‚’è¨ˆç®—
        word_count = len(article.content) if article.content else 0
        
        # å…¬é–‹æ—¥ã®å‡¦ç†
        published_date = article.published_at if article.published_at else article.created_at
        if published_date:
            published_iso = published_date.isoformat()
        else:
            published_iso = datetime.now().isoformat()
        
        # JSON-LDæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
        structured_data = {
            "@context": "https://schema.org",
            "@type": "BlogPosting",
            "headline": article.title or "Untitled",
            "description": article.summary or article.title or "No description available",
            "author": {
                "@type": "Person",
                "name": author_name
            },
            "datePublished": published_iso,
            "dateModified": article.updated_at.isoformat() if article.updated_at else published_iso,
            "wordCount": word_count,
            "articleSection": categories,
            "keywords": categories,
            "publisher": {
                "@type": "Organization",
                "name": "Miyakawa.codes",
                "logo": {
                    "@type": "ImageObject",
                    "url": "https://miyakawa.codes/static/images/logo.png"
                }
            }
        }
        
        # ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if hasattr(article, 'featured_image_url') and article.featured_image_url:
            structured_data["image"] = {
                "@type": "ImageObject",
                "url": f"https://miyakawa.codes{article.featured_image_url}"
            }
        
        return json.dumps(structured_data, ensure_ascii=False, indent=2)
        
    except Exception as e:
        current_app.logger.error(f"Error generating structured data: {e}")
        return None