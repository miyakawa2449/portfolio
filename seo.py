"""
SEO/OGP Functions - SEOæœ€é©åŒ–ã¨Open Graph Protocolé–¢é€£æ©Ÿèƒ½
"""
import re
import time
import hashlib
import json
import requests
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs
from bs4 import BeautifulSoup
from flask import current_app, url_for

# OGPãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¡ãƒ¢ãƒªï¼‰
ogp_cache = {}
OGP_CACHE_DURATION = 3600  # 1æ™‚é–“

def process_sns_auto_embed(text):
    """ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®SNS URLã‚’è‡ªå‹•çš„ã«åŸ‹è¾¼HTMLã«å¤‰æ›"""
    if not text:
        return text
    
    # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®HTMLã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
    if any(cls in text for cls in ['sns-embed', 'youtube-embed', 'twitter-embed', 'instagram-embed', 'facebook-embed', 'threads-embed', 'ogp-card']):
        current_app.logger.debug("ğŸš« Already processed content detected, skipping SNS auto embed")
        return text
    
    current_app.logger.debug(f"ğŸ” Processing SNS auto embed for text length: {len(text)}")
    
    # SNSãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆURLã‚’ãƒãƒƒãƒï¼‰
    sns_patterns = {
        'youtube': [
            r'(https?://(?:www\.)?youtube\.com/watch\?v=([a-zA-Z0-9_-]+)(?:\S*)?)',
            r'(https?://youtu\.be/([a-zA-Z0-9_-]+)(?:\?\S*)?)'
        ],
        'twitter': [
            r'(https?://(?:www\.)?twitter\.com/[a-zA-Z0-9_]+/status/(\d+)(?:\S*)?)',
            r'(https?://(?:www\.)?x\.com/[a-zA-Z0-9_]+/status/(\d+)(?:\S*)?)',
        ],
        'instagram': [
            r'(https?://(?:www\.)?instagram\.com/p/([a-zA-Z0-9_-]+)/?(?:\?\S*)?)',
            r'(https?://(?:www\.)?instagram\.com/reel/([a-zA-Z0-9_-]+)/?(?:\?\S*)?)'
        ],
        'facebook': [
            r'(https?://(?:www\.)?facebook\.com/\w+/posts/(\d+)(?:\S*)?)',
            r'(https?://(?:www\.)?facebook\.com/\w+/videos/(\d+)(?:\S*)?)',
            r'(https?://fb\.watch/([a-zA-Z0-9_-]+)/?(?:\?\S*)?)'
        ],
        'threads': [
            r'(https?://(?:www\.)?threads\.net/@\w+/post/([a-zA-Z0-9_-]+)(?:\S*)?)',
            r'(https?://(?:www\.)?threads\.com/@\w+/post/([a-zA-Z0-9_-]+)(?:\S*)?)'
        ]
    }
    
    # å„ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ç½®æ›
    for platform, patterns in sns_patterns.items():
        for pattern in patterns:
            # ãƒ‡ãƒãƒƒã‚°: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã®ãƒ†ã‚¹ãƒˆ
            matches = re.findall(pattern, text)
            if matches:
                current_app.logger.debug(f"ğŸ“± Found {len(matches)} {platform} URLs: {matches}")
            
            def replace_match(match):
                url = match.group(1).strip()  # ã‚°ãƒ«ãƒ¼ãƒ—1ãŒURLå…¨ä½“
                current_app.logger.debug(f"ğŸ”„ Converting {platform} URL: {url[:50]}...")
                
                if platform == 'youtube':
                    return generate_youtube_embed(url)
                elif platform == 'twitter':
                    return generate_twitter_embed(url)
                elif platform == 'instagram':
                    return generate_instagram_embed(url)
                elif platform == 'facebook':
                    return generate_facebook_embed(url)
                elif platform == 'threads':
                    return generate_threads_embed(url)
                else:
                    return url  # å¤‰æ›ã§ããªã„å ´åˆã¯å…ƒã®URLã‚’è¿”ã™
            
            # URLãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹å…¨ã¦ã®URLã‚’å¯¾è±¡
            text = re.sub(pattern, replace_match, text)
    
    # ä¸€èˆ¬çš„ãªWebã‚µã‚¤ãƒˆURLã®OGPã‚«ãƒ¼ãƒ‰è¡¨ç¤ºå‡¦ç†ã‚’è¿½åŠ 
    text = process_general_url_embeds(text)
    
    return text

def process_general_url_embeds(text):
    """ä¸€èˆ¬çš„ãªWebã‚µã‚¤ãƒˆURLã‚’OGPã‚«ãƒ¼ãƒ‰è¡¨ç¤ºã«å¤‰æ›"""
    if not text:
        return text
    
    # ä¸€èˆ¬çš„ãªURLæ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç‹¬ç«‹è¡Œã§ã€ã‹ã¤SNSã§ã¯ãªã„URLï¼‰
    # SNSãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’é™¤å¤–ã™ã‚‹ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ«ãƒƒã‚¯ã‚¢ãƒ˜ãƒƒãƒ‰
    general_url_pattern = r'^(https?://(?!(?:www\.)?(youtube\.com|youtu\.be|twitter\.com|x\.com|instagram\.com|facebook\.com|fb\.watch|threads\.net|threads\.com))[^\s]+)$'
    
    def replace_general_url(match):
        url = match.group(1).strip()
        return generate_ogp_card(url)
    
    # è¡Œå˜ä½ã§URLã‚’æ¤œå‡ºã—ã¦ç½®æ›
    text = re.sub(general_url_pattern, replace_general_url, text, flags=re.MULTILINE)
    
    current_app.logger.debug(f"âœ… SNS auto embed processing completed. Output length: {len(text)}")
    return text

def detect_platform_from_url(url):
    """URLã‹ã‚‰ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’æ¤œå‡º"""
    if 'youtube.com' in url or 'youtu.be' in url:
        return 'youtube'
    elif 'twitter.com' in url or 'x.com' in url:
        return 'twitter'
    elif 'instagram.com' in url:
        return 'instagram'
    elif 'facebook.com' in url or 'fb.watch' in url:
        return 'facebook'
    elif 'threads.com' in url or 'threads.net' in url:
        return 'threads'
    else:
        return 'general'

def generate_youtube_embed(url):
    """YouTube URLã‚’åŸ‹è¾¼HTMLã«å¤‰æ›"""
    video_id_patterns = [
        r'(?:youtube\.com/watch\?v=)([a-zA-Z0-9_-]+)',
        r'(?:youtu\.be/)([a-zA-Z0-9_-]+)'
    ]
    
    video_id = None
    for pattern in video_id_patterns:
        match = re.search(pattern, url)
        if match:
            video_id = match.group(1)
            break
    
    if video_id:
        return f'''<div class="sns-embed youtube-embed">
    <iframe width="100%" height="315" src="https://www.youtube-nocookie.com/embed/{video_id}" 
            title="YouTube video player" frameborder="0" allowfullscreen>
    </iframe>
</div>'''
    else:
        return f'<a href="{url}" target="_blank">{url}</a>'

def generate_twitter_embed(url):
    """Twitter/X URLã‚’åŸ‹è¾¼HTMLã«å¤‰æ›"""
    # JavaScriptåŸ‹è¾¼ã®ä»£ã‚ã‚Šã«OGPã‚«ãƒ¼ãƒ‰å½¢å¼ã‚’ä½¿ç”¨
    return generate_ogp_card(url)

def generate_instagram_embed(url):
    """Instagram URLã‚’åŸ‹è¾¼HTMLã«å¤‰æ›"""
    return f'''<div class="sns-embed instagram-embed">
    <blockquote class="instagram-media" data-instgrm-permalink="{url}">
        <a href="{url}" target="_blank">{url}</a>
    </blockquote>
    <script async src="https://www.instagram.com/embed.js"></script>
</div>'''

def generate_facebook_embed(url):
    """Facebook URLã‚’åŸ‹è¾¼HTMLã«å¤‰æ›"""
    return f'''<div class="sns-embed facebook-embed">
    <div class="fb-post" data-href="{url}"></div>
</div>'''

def generate_threads_embed(url):
    """Threads URLã‚’åŸ‹è¾¼HTMLã«å¤‰æ›"""
    return f'''<div class="sns-embed threads-embed">
    <blockquote class="threads-post">
        <a href="{url}" target="_blank">{url}</a>
    </blockquote>
</div>'''

def fetch_ogp_data(url, force_refresh=False):
    """URLã‹ã‚‰OGPï¼ˆOpen Graph Protocolï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œã€Seleniumå¯¾å¿œï¼‰"""
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    cache_key = hashlib.md5(url.encode()).hexdigest()
    current_time = datetime.now()
    
    # force_refreshãŒTrueã®å ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if not force_refresh and cache_key in ogp_cache:
        cached_data, cached_time = ogp_cache[cache_key]
        if current_time - cached_time < timedelta(seconds=OGP_CACHE_DURATION):
            current_app.logger.debug(f"OGP cache hit for: {url[:50]}...")
            return cached_data
    
    # Threads URLã‹ã©ã†ã‹ã‚’åˆ¤å®š
    is_threads_url = 'threads.com' in url or 'threads.net' in url
    
    try:
        if is_threads_url:
            # Threadsã«ã¯Seleniumã‚’ä½¿ç”¨
            ogp_data = _fetch_threads_ogp_with_selenium(url)
        else:
            # é€šå¸¸ã®URLã«ã¯å¾“æ¥ã®æ–¹æ³•ã‚’ä½¿ç”¨
            ogp_data = _fetch_ogp_with_requests(url)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        ogp_cache[cache_key] = (ogp_data, current_time)
        current_app.logger.debug(f"OGP data cached for: {url[:50]}...")
        
        return ogp_data
        
    except Exception as e:
        current_app.logger.error(f"OGP fetch error: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ç©ºã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆçŸ­æ™‚é–“ï¼‰
        empty_data = {}
        ogp_cache[cache_key] = (empty_data, current_time)
        return empty_data

def _fetch_ogp_with_requests(url):
    """é€šå¸¸ã®HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§OGPãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate'
    }
    
    try:
        current_app.logger.debug(f"ğŸŒ Fetching OGP for: {url[:50]}...")
        start_time = time.time()
        
        response = requests.get(url, headers=headers, timeout=8, stream=True)
        response.raise_for_status()
        
        # æœ€åˆã®64KBã®ã¿ã‚’èª­ã¿å–ã‚‹ï¼ˆOGPã¯HTMLãƒ˜ãƒƒãƒ€ã«ã‚ã‚‹ãŸã‚ï¼‰
        content_size_limit = 65536  # 64KB
        content = b''
        for chunk in response.iter_content(chunk_size=8192):
            content += chunk
            if len(content) >= content_size_limit:
                break
        response.close()
        
        soup = BeautifulSoup(content, 'html.parser')
        
        ogp_data = {}
        
        # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªOGPãƒ¡ã‚¿ã‚¿ã‚°æ¤œç´¢
        ogp_tags = soup.find_all('meta', attrs={'property': lambda x: x and x.startswith('og:')})
        twitter_tags = soup.find_all('meta', attrs={'name': lambda x: x and x.startswith('twitter:')})
        
        # OGPã‚¿ã‚°ã®å‡¦ç†
        for tag in ogp_tags:
            prop = tag.get('property', '').lower()
            content = tag.get('content', '').strip()
            if content:
                if prop == 'og:title':
                    ogp_data['title'] = content
                elif prop == 'og:description':
                    ogp_data['description'] = content
                elif prop == 'og:image':
                    ogp_data['image'] = content
                elif prop == 'og:site_name':
                    ogp_data['site_name'] = content
                elif prop == 'og:url':
                    ogp_data['url'] = content
        
        # Twitterã‚«ãƒ¼ãƒ‰ã‚¿ã‚°ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨
        for tag in twitter_tags:
            name = tag.get('name', '').lower()
            content = tag.get('content', '').strip()
            if content:
                if name == 'twitter:title' and not ogp_data.get('title'):
                    ogp_data['title'] = content
                elif name == 'twitter:description' and not ogp_data.get('description'):
                    ogp_data['description'] = content
                elif name == 'twitter:image' and not ogp_data.get('image'):
                    ogp_data['image'] = content
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®metaã‚¿ã‚°ã‹ã‚‰ã‚‚å–å¾—
        if not ogp_data.get('title'):
            title_tag = soup.find('title')
            if title_tag:
                ogp_data['title'] = title_tag.get_text().strip()
        
        if not ogp_data.get('description'):
            desc_tag = soup.find('meta', attrs={'name': 'description'})
            if desc_tag and desc_tag.get('content'):
                ogp_data['description'] = desc_tag.get('content', '').strip()
        
        # ã‚µã‚¤ãƒˆåãŒãªã„å ´åˆã¯ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰æ¨æ¸¬
        if not ogp_data.get('site_name'):
            parsed_url = urlparse(url)
            domain = parsed_url.netloc.replace('www.', '')
            ogp_data['site_name'] = domain
        
        fetch_time = time.time() - start_time
        current_app.logger.debug(f"âœ… OGP fetched in {fetch_time:.2f}s for: {url[:50]}...")
        
        return ogp_data
        
    except requests.exceptions.Timeout:
        current_app.logger.warning(f"â° OGP timeout for: {url[:50]}...")
        return {}
    except requests.exceptions.RequestException as e:
        current_app.logger.warning(f"âš ï¸ OGP request failed for {url[:50]}...: {e}")
        return {}
    except Exception as e:
        current_app.logger.error(f"âŒ OGP fetch error for {url[:50]}...: {e}")
        return {}

def _fetch_threads_ogp_with_selenium(url):
    """Seleniumã§Threadsã®OGPãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
    import time
    
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    
    driver = None
    try:
        current_app.logger.debug("ğŸŒ Starting Selenium for Threads URL...")
        
        # webdriver-managerã§ChromeDriverã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        import os
        import stat
        
        # ChromeDriverManagerã®ä»£ã‚ã‚Šã«ç›´æ¥ãƒ‘ã‚¹ã‚’æŒ‡å®š
        base_wdm_path = os.path.expanduser("~/.wdm/drivers/chromedriver")
        
        # webdriver-managerã‚’ä½¿ç”¨ã—ã¦ãƒ‘ã‚¹ã‚’å–å¾—
        try:
            driver_path = ChromeDriverManager().install()
            current_app.logger.debug(f"ChromeDriver manager path: {driver_path}")
            
            # webdriver-managerãŒé–“é•ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã—ã¦ã„ã‚‹å ´åˆã®ä¿®æ­£
            driver_dir = os.path.dirname(driver_path)
            chromedriver_path = os.path.join(driver_dir, "chromedriver")
            
            # æ­£ã—ã„chromedriverå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹
            if os.path.exists(chromedriver_path) and os.path.isfile(chromedriver_path):
                actual_driver_path = chromedriver_path
            else:
                # å†å¸°çš„ã«chromedriverå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
                import glob
                pattern = os.path.join(base_wdm_path, "**/chromedriver")
                found_drivers = glob.glob(pattern, recursive=True)
                for found_driver in found_drivers:
                    if os.path.isfile(found_driver):
                        actual_driver_path = found_driver
                        break
            
            if not actual_driver_path:
                raise Exception("Could not find valid ChromeDriver executable")
                
        except Exception as e:
            current_app.logger.error(f"ChromeDriverManager failed: {e}")
            raise Exception("Could not find valid ChromeDriver executable")
        
        current_app.logger.debug(f"Actual ChromeDriver path: {actual_driver_path}")
        
        # å®Ÿè¡Œæ¨©é™ã‚’ç¢ºèªãƒ»è¨­å®š
        if not os.access(actual_driver_path, os.X_OK):
            os.chmod(actual_driver_path, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR | stat.S_IRGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)
            current_app.logger.debug("Set executable permission for ChromeDriver")
        
        service = Service(actual_driver_path)
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.get(url)
        
        # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ
        current_app.logger.debug("â³ Waiting for page load...")
        time.sleep(5)
        
        # OGPãƒ¡ã‚¿ã‚¿ã‚°ãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, "//meta[contains(@property, 'og:') or contains(@name, 'twitter:')]"))
            )
            current_app.logger.debug("âœ… OGP meta tags detected")
        except:
            current_app.logger.debug("âš ï¸ OGP meta tags not found, continuing anyway")
        
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        
        ogp_data = {}
        
        # OGPã¨Twitterã‚«ãƒ¼ãƒ‰ã®æƒ…å ±ã‚’å–å¾—
        for tag in soup.find_all('meta'):
            for attr in ['property', 'name']:
                if tag.has_attr(attr):
                    key = tag.get(attr)
                    content = tag.get('content', '')
                    if key and content:
                        if key.startswith('og:'):
                            if key == 'og:title':
                                ogp_data['title'] = content
                            elif key == 'og:description':
                                ogp_data['description'] = content
                            elif key == 'og:image':
                                ogp_data['image'] = content
                            elif key == 'og:site_name':
                                ogp_data['site_name'] = content
                            elif key == 'og:url':
                                ogp_data['url'] = content
                        elif key.startswith('twitter:'):
                            if key == 'twitter:title' and not ogp_data.get('title'):
                                ogp_data['title'] = content
                            elif key == 'twitter:description' and not ogp_data.get('description'):
                                ogp_data['description'] = content
                            elif key == 'twitter:image' and not ogp_data.get('image'):
                                ogp_data['image'] = content
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: HTMLã‹ã‚‰ã®åŸºæœ¬æƒ…å ±å–å¾—
        if not ogp_data.get('title'):
            title_tag = soup.find('title')
            if title_tag:
                ogp_data['title'] = title_tag.get_text().strip()
        
        if not ogp_data.get('description'):
            desc_tag = soup.find('meta', attrs={'name': 'description'})
            if desc_tag:
                content = desc_tag.get('content', '')
                if content:
                    ogp_data['description'] = content
        
        # Threadsç‰¹æœ‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
        if not ogp_data.get('title') or ogp_data.get('title') == 'Threads':
            import re
            user_match = re.search(r'@([^/]+)/', url)
            if user_match:
                username = user_match.group(1)
                ogp_data['title'] = f"{username} (@{username}) on Threads"
                if not ogp_data.get('description'):
                    ogp_data['description'] = f"@{username}ã®æŠ•ç¨¿ã‚’Threadsã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                ogp_data['site_name'] = 'Threads'
        
        current_app.logger.debug(f"ğŸ“Š Selenium fetched {len(ogp_data)} meta items for Threads")
        return ogp_data
        
    except Exception as e:
        current_app.logger.error(f"âŒ Selenium fetch failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: URLã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º
        import re
        user_match = re.search(r'@([^/]+)/', url)
        if user_match:
            username = user_match.group(1)
            return {
                'title': f"{username} (@{username}) on Threads",
                'description': f"@{username}ã®æŠ•ç¨¿ã‚’Threadsã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                'site_name': 'Threads'
            }
        return {}
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

def generate_ogp_card(url):
    """URLã®OGPã‚«ãƒ¼ãƒ‰HTMLã‚’ç”Ÿæˆ"""
    try:
        ogp_data = fetch_ogp_data(url)
        
        if not ogp_data:
            # OGPãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å ´åˆã¯ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªãƒ³ã‚¯ã‚’è¿”ã™
            return f'<a href="{url}" target="_blank" class="simple-link">{url}</a>'
        
        # ã‚µã‚¤ãƒˆåã®çŸ­ç¸®è¡¨ç¤º
        site_name = ogp_data.get('site_name', '')
        if len(site_name) > 20:
            site_name = site_name[:17] + '...'
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®çŸ­ç¸®è¡¨ç¤º
        title = ogp_data.get('title', url)
        if len(title) > 80:
            title = title[:77] + '...'
        
        # èª¬æ˜æ–‡ã®çŸ­ç¸®è¡¨ç¤º
        description = ogp_data.get('description', '')
        if len(description) > 120:
            description = description[:117] + '...'
        
        # ç”»åƒURLã®å‡¦ç†
        image_url = ogp_data.get('image', '')
        image_html = ''
        if image_url:
            # ç›¸å¯¾URLã®å ´åˆã¯çµ¶å¯¾URLã«å¤‰æ›
            if image_url.startswith('/'):
                from urllib.parse import urljoin
                image_url = urljoin(url, image_url)
            image_html = f'<img src="{image_url}" alt="{title}" onerror="this.style.display=\'none\'">'
        
        # OGPã‚«ãƒ¼ãƒ‰HTMLã®ç”Ÿæˆ
        card_html = f'''<div class="ogp-card" style="border: 1px solid #e1e8ed; border-radius: 12px; overflow: hidden; margin: 16px 0; max-width: 500px; text-decoration: none; color: inherit;">
    <a href="{url}" target="_blank" style="text-decoration: none; color: inherit; display: block;">
        {f'<div class="ogp-image" style="width: 100%; height: 200px; overflow: hidden; background: #f7f9fa;">{image_html}</div>' if image_url else ''}
        <div class="ogp-content" style="padding: 16px;">
            <div class="ogp-title" style="font-weight: bold; font-size: 16px; line-height: 1.3; margin-bottom: 8px; color: #14171a;">{title}</div>
            {f'<div class="ogp-description" style="font-size: 14px; line-height: 1.4; color: #657786; margin-bottom: 8px;">{description}</div>' if description else ''}
            <div class="ogp-site" style="font-size: 12px; color: #657786; text-transform: uppercase;">{site_name}</div>
        </div>
    </a>
</div>'''
        
        return card_html
        
    except Exception as e:
        current_app.logger.error(f"Error generating OGP card: {e}")
        return f'<a href="{url}" target="_blank" class="simple-link">{url}</a>'

def generate_article_structured_data(article):
    """è¨˜äº‹ç”¨ã®JSON-LDæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    try:
        # ä½œæˆè€…æƒ…å ±ã‚’å®‰å…¨ã«å–å¾—
        author_name = "Unknown Author"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        if hasattr(article, 'user') and article.user:
            if hasattr(article.user, 'display_name') and article.user.display_name:
                author_name = article.user.display_name
            elif hasattr(article.user, 'email') and article.user.email:
                author_name = article.user.email
        
        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—
        categories = []
        if hasattr(article, 'categories') and article.categories:
            categories = [cat.name for cat in article.categories if hasattr(cat, 'name')]
        
        # è¨˜äº‹ã®æ–‡å­—æ•°ã‚’è¨ˆç®—
        word_count = len(article.body) if article.body else 0
        
        # å…¬é–‹æ—¥ã®å‡¦ç†
        published_date = article.published_at if article.published_at else article.created_at
        if published_date:
            published_iso = published_date.isoformat()
        else:
            from datetime import datetime
            published_iso = datetime.now().isoformat()
        
        # JSON-LDæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
        structured_data = {
            "@context": "https://schema.org",
            "@type": "BlogPosting",
            "headline": article.title or "Untitled",
            "description": article.summary or article.title or "No description available",
            "author": {
                "@type": "Person",
                "name": author_name
            },
            "datePublished": published_iso,
            "dateModified": article.updated_at.isoformat() if article.updated_at else published_iso,
            "wordCount": word_count,
            "articleSection": categories,
            "keywords": categories,
            "publisher": {
                "@type": "Organization",
                "name": "Miyakawa.codes",
                "logo": {
                    "@type": "ImageObject",
                    "url": "https://miyakawa.codes/static/images/logo.png"
                }
            }
        }
        
        # ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if hasattr(article, 'featured_image_url') and article.featured_image_url:
            structured_data["image"] = {
                "@type": "ImageObject",
                "url": f"https://miyakawa.codes{article.featured_image_url}"
            }
        
        return json.dumps(structured_data, ensure_ascii=False, indent=2)
        
    except Exception as e:
        current_app.logger.error(f"Error generating structured data: {e}")
        return None

def get_static_page_seo(page_name):
    """é™çš„ãƒšãƒ¼ã‚¸ç”¨ã®SEOè¨­å®šã‚’å–å¾—"""
    seo_configs = {
        'home': {
            'title': 'Miyakawa.codes - Python 100 Days Challenge Portfolio',
            'description': 'Python 100æ—¥ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã®å­¦ç¿’è¨˜éŒ²ã¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæˆæœã‚’ã¾ã¨ã‚ãŸãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚µã‚¤ãƒˆã§ã™ã€‚',
            'keywords': 'Python, ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°, 100æ—¥ãƒãƒ£ãƒ¬ãƒ³ã‚¸, ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª, å­¦ç¿’è¨˜éŒ²',
            'og_type': 'website'
        },
        'blog': {
            'title': 'ãƒ–ãƒ­ã‚° - Pythonå­¦ç¿’è¨˜éŒ²',
            'description': 'Python 100æ—¥ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã®æ—¥ã€…ã®å­¦ç¿’è¨˜éŒ²ã¨ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®æˆé•·éç¨‹ã‚’è¨˜éŒ²ã—ã¦ã„ã¾ã™ã€‚',
            'keywords': 'Python, ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å­¦ç¿’, ãƒ–ãƒ­ã‚°, 100æ—¥ãƒãƒ£ãƒ¬ãƒ³ã‚¸',
            'og_type': 'blog'
        },
        'projects': {
            'title': 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ - ä½œå“ã‚®ãƒ£ãƒ©ãƒªãƒ¼',
            'description': 'Python 100æ—¥ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã§ä½œæˆã—ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨ä½œå“ã®ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã§ã™ã€‚',
            'keywords': 'Python, ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ, ä½œå“, ã‚®ãƒ£ãƒ©ãƒªãƒ¼, ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª',
            'og_type': 'website'
        }
    }
    
    return seo_configs.get(page_name, {
        'title': 'Miyakawa.codes',
        'description': 'Pythonå­¦ç¿’ã¨ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®è¨˜éŒ²',
        'keywords': 'Python, ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°',
        'og_type': 'website'
    })